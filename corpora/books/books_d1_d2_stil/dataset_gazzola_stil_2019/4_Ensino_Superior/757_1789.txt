 Para melhor compreender o comportamento de uma aplicação paralela Hadoop é importante entender os passos de sua execução. Esses passos são ilustrados na figura abaixo e descritos a seguir. Os números entre parênteses se referem aos passos na figura. Ao disparar o programa da aplicação, o Hadoop dispara os processos que farão parte da execução (1). Um processo mestre, responsável por coordenar os demais, é disparado junto à aplicação original do usuário e processos trabalhadores são disparados no conjunto de máquinas configurados durante a instalação do Hadoop. Com base na informação sobre o arquivo de entrada, o mestre identifica os trabalhadores que foram disparados mais próximo de cópias dos blocos do arquivo e atribui a cada um deles parte das tarefas de "map", identificando os pedaços do arquivo que devem ser processados por cada trabalhador, denominados "splits" (2). Os trabalhadores começam então a ler pedaços do arquivo e produzir os pares chave/valor para a tarefas de "map" (3). Os pares chave/valor produzidos nessa etapa são assinalados para um dos processos trabalhadores escolhidos para a redução por uma função de assinalamento. Essa função é normalmente uma função de "hash" simples, parametrizada pelo número de redutores, mas pode ser substituída por outro tipo de mapeamento definido pelo usuário. O resultado do "map" é então armazenado localmente na máquina em que cada trabalhador executa, agrupado pelos identificadores dos processo de redução assinalados pela função de mapeamento (4). À medida que tarefas de mapeamento sobre "splits" completam, os trabalhadores informam o mestre sobre quais chaves foram geradas no processo e identificam os arquivos gerados para cada nó responsável pela redução (5). O mestre, à medida que cada "split" do arquivo de entrada é completado, coleta a informação sobre os arquivos intermediários e notifica cada nó trabalhador que executará um "reduce" sobre quais nós de "map" já têm arquivos intermediários com chaves destinadas a eles (6). Os redutores então contactam os nós identificados e vão requisitando esses arquivos usando consultas HTTP. Os pares chave/valor recebidos vão sendo ordenados por suas chaves, para garantir a geração correta das listas de valor para cada chave de forma completa (7). Quando os trabalhadores que executam as tarefas de mapeamento terminam e todos os pares chave/valor intermediários estão disponíveis para os processos de redução adequados, a função "reduce" começa a ser executada para cada chave assinalada para um trabalhador pela função de mapeamento. A saída de cada trabalhador envolvido na redução é escrita em um arquivo individual no HDFS, todos no mesmo diretório indicado pelo usuário na configuração da aplicação (8). O número de trabalhadores pode ser configurado pelo usuário, mas em geral é calculado automaticamente pelo Hadoop, em função do tamanho da entrada (número de "splits") e do número de máquinas disponíveis. Como no MapReduce original, Hadoop usa replicação de tarefas para conseguir balanceamento de carga e tolerância a falhas. À medida que as tarefas de um certo tipo vão terminando, o processo mestre dispara novas execuções de tarefas que estão levando mais tempo para serem completadas em outros nós. Além disso, ele monitora o estado de todos os trabalhadores a fim de detectar nós que tenham deixado de responder. Nesse caso, ele identifica todas as tarefas que haviam sido atribuídas a um nó (sejam "maps" ou "reduces") e as atribui novamente a outros nós ainda ativos.