 A configuração do cluster onde trabalhamos foi feita a partir do OpenStack , software para fácil gerenciamento de infraestruturas virtualizadas. Nosso cluster continha quatro máquinas virtuais diferentes. Para integração das plataformas, substituímos o sistema Java openSDK das máquinas virtuais ubuntu pelo sistema da Sun - para o qual o Hadoop é melhor adaptado. Utilizamos um namenode com réplica e quatro datanodes com nível de redundância igual a 2. Utilizamos a API Java do Hadoop para criarmos nossas tarefas de MapReduce, assim como o nosso driver para iniciação dos Jobs. Note que a dificuldade que temos aqui consiste em passar os novos valores de probabilidade para os maps a cada iteração. Note que precisamos passar somente P(cara), uma vez que P(coroa) pode ser dado por 1-P(cara). Para isso, utilizamos do JobConf. O JobConf é acessível a todo processo de map e reduce e permite que lhe sejam atribuídos parâmetros com valores de tipos primitivos e recuperá-los durante a execução através do método get(). public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, IntWritable, DoubleArrayWritable>{
A tarefa de reduce se encarrega de receber as contagens de arremessos de acordo com o número da moeda e gerar a nova probabilidade de P(cara) para cada moeda. public void reduce(IntWritable moeda, Iterator<DoubleArrayWritable> lancamentos, OutputCollector<IntWritable, DoubleWritable>output, Reporter reporter){
O nosso driver é por onde inicia-se a execução do programa. Uma vez iniciada, a cada iteração testa-se a convergência, isto é, se a diferença entre os valore de P(cara) de cada uma das novas moedas entre iterações é menor que certo limiar. public static void main(String[] args) throws Exception{
A carga de trabalho foi gerada artificialmente. A divisão do trabalho é feita com base no conjunto de lançamentos e, portanto, um grande número de conjuntos de lançamentos foram gerados aleatoriamente para duas moedas, uma com P(cara) = 0.7 e outra com P(cara) = 0.4. O Objetivo é conseguir agrupar os conjuntos de lançamentso nos seus próprios cluster, numa mistura gaussiana, descobridno os parâmetros P(cara) de cada moeda. Foram gerados 100,000,000,000 que foram escritos em um arquivo de texto. Cada linha representa um lançamento contendo dois inteiros: o número de caras e o número de coroas. A representação de todos os lançamentos utilizando dois inteiros de 4 bytes para cada lançamento utiliza aproximadamente 0.75Gb. A avaliação foi feita inicialmente sobre um single-node cluster rodando localmente e depois se expande para o cluster com quatro máquinas, sendo um namenode que também funciona como datanode. O arquivo contendo a lista de lançamentos é carregada inicialmente de forma a já ser particionada entre os nós antes da execução das tarefas. Mede-se o tempo de execução até a convergência para uma versão serial da aplicação e compara-se com os resultados alcançados com o algoritmo distribuído. [andamento]
Para um nó o processamento é local. Note que o speedup é quase de 2 vezes aumentando para dois nós, mas, no entanto, ele não é tão grande aumentando-se mais nós. Isso se deve ao fato o tamanho da ram da máquina (2Gb) ser o fato limitante para processamento dos 100,000,000 pontos. Dividindo-se o workload em duas máquinas, divide-se também a quantidade de memória necessária para o processamento. No entanto, a memória deixa, então , de ser o principal gargalo. A vantagem do uso do Hadoop foi em dividir os dados desde o início utilizando o sistema distribuído e seguir a filosofia de "mandar o processamento para onde temos dados". No entanto, se tívessemos que distribuir os dados enquanto exetávamos o algoritmo de EM, provavelmente não teríamos obtidos resultados tão bons, uma vez que a rede seria um gargalo para transmissão de dados.