 Fora o particionamento da base, o algoritmo possui três fases: (1) mineração de itemsets frequentes locais em cada partição; (2) agregação das contagens locais usando a união dos itemsets frequentes maximais gerados na etapa anterior -- chamados "Upper Bounds"; e (3) soma final e filtragem pelo suporte. Nesse exemplo é possível verificar que, como o algoritmo processa uma partição por vez e os resultados intermediários são gravados em disco, pode-se processar uma base de transações contendo oito transações em uma memória principal que comporta apenas quatro. Nessa seção, apresentam-se os requisitos de escalabilidade, armazenamento, latência e tolerância a falhas considerados no projeto. Ao duplicar o número de processadores disponíveis para processamento, é desejável que o processamento execute duas vezes mais rápido. Isto é, almeja-se obter um speed-up próximo ao linear. De fato, esse resultado foi alcançado pela abordagem aqui apresentada. Em muitos cenários reais o volume de dados é tal que não é possível armazená-lo em apenas uma máquina. Sendo necessário um conjunto de máquinas para comportar esse volume. Esse é o cenário ideal para análise da aplicação aqui desenvolvida. Consideramos nesse projeto que a identificação de itemsets frequentes seja um processo offline e portanto não assumimos nenhum requisito de latência. Deseja-se porém obter speed-up próximo ao linear. Dado o grande volume de dados e o tempo necessário para processamento, é comum que aplicações de big-data possuam tolerancia a falhas. Embora na presente aplicação tal quesito também seja importante, não entraremos no mérito de propor estratégias de tolerancia a falhas nesse trabalho. Diversos frameworks implementam tolerancia a falhas, evitando que o programador tenha tais preocupaçoes. Portanto, deixaremos isso por conta do framework a ser utilizado. Além disso, a existencia ou não de tolerancia a falhas não está intrinsecamente relacionada ao algoritmo. Assim, para o contexto em questão assumimos que isso não é crítico. = Projeto =
Conforme já mencionado em seções anteriores, é comum que aplicações de mineração de dados dependam da extração de itemsets frequentes em grandes volumes de dados.