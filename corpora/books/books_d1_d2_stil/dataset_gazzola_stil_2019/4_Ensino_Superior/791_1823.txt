 Na maioria das aplicações voltadas para dados massivos, a tolerância de falhas é satisfeita com a replicação de registros em diferentes nós e com a projeção de algoritmos que não possuem um único ponto de falha. Esta seção descreve como o a implementação do "DBScan" já foi realizada para suportar grandes volumes de dados. Em é apresentado uma implementação paralela do "DBScan" com uma abordagem mestre-escravo: enquanto o núcleo mestre realiza a etapa de assimilação de grupos, os escravos respondem a consultas de vizinhança usando a estrutura "R*-Tree" para armazenamento. Em "P-DBSCAN" , a base é particionada e o agrupamento é feito de forma independente entre os nós de forma distribuída. Ao final, há uma agregação dos resultados de cada nó para formar o resultado final. Quanto ao armazenamento, a estrutura utilizada é a "Priority R-Tree" que é uma variação eficiente da "R-Tree". Nessa implementação há a limitação de haver um único nó para juntar os resultados do agrupamento feito por todos os nós. Além disso, os pontos considerados exceções por um nó não são tratados posteriormente na junção dos grupos, portanto grupos densos podem ser perdidos se seus registros estiverem divididos entre os nós. De forma similar ao "P-DBSCAN", o "MR-DBSCAN" , proposto em , é uma implementação distribuída do "DBScan" com quatro estágios e que utiliza o paradigma "Map-reduce" . A primeira etapa consiste em dividir a base entre os nós de forma balanceada e de forma a deixar os registros mais próximos no mesmo nó. Em seguida, na fase "map", o "DBScan" é executado de forma independente dentro de cada nó. A terceira etapa é a fase "reduce": todos os nós são analisados para descobrir em quais situações o mesmo nó foi agrupado para diferentes grupos, ou seja, é feito um mapeamento da junção e remarcação dos grupos que é realizada na quarta e última etapa. Os resultados mostraram que a escalabilidade e a eficiência dessa abordagem são bastante satisfatórias.