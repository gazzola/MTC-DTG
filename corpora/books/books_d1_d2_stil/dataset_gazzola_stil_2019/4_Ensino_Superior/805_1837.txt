 Processamento de Dados Massivos/Projeto e implementação de aplicações Big Data/Maximização de expectativas
=Maximização de Expectativas=
Nesta seção apresentamos o algoritmo conhecido como Maximização de Expectativas, em especial sua utilização em ambientes distribuídos que permitem o processamento de dados massivos, ou "big data". Esta seção se divide em cinco seções a seguir: Descrição, Projeto, Desenvolvimento, Implementação e Avaliação. Finalmente, fazemos um breve estudo de caso do algoritmo, quando foi aplicado em uma competição, também de "big data". Aqui iremos descrever o algoritmo conhecido como Maximização de Expectativas que, em conjunto com a maneira como iremos aplicá-lo ao cenário distribuído para o tratamento de dados massivos. O algoritmo Maximização de Expectativas, mais conhecido pelo seu nome inglês "Expectation Maximization" ou EM, pertece a uma classe de técnicas estatísticas para a estimativa de parâmetros para modelos estatísticos quando existem variáveis latentes (ou escondidas). Os parâmetros encontrados são estimativas por máxima verossimilhança ou "maximum likelihood estimates (MLE)". Por modelo estatísticos estamos nos referindo à, basicamente, uma equação que descreve relações entre variáveis através de distribuições de probabilidade. Essas distribuições tornam as relações estocásticas ao invés de deterministas. Já variáveis latentes são variáveis que não são diretamente observadas nos dados. Dessa maneira, elas devem ser inferidas baseado nas variáveis que foram de fato observadas. A “mágica” do EM é que ele é capaz de lidar tanto com as variáveis latentes quanto com os parâmetros simultaneamente. Finalmente, a verossimilhança de um conjunto de parâmetros é, basicamente, a probabilidade desses parâmetros gerarem os resultados obtidos. O MLE é um método de inferência dos parâmetros com verossimilhança (ou probabilidade) máxima. O EM é uma generalização, portanto, do MLE em cenários que existem variáveis latentes. É usual se utilizar o logaritmo da verossimilhança (ou "log-likelihood"), porque a função logaritmica é uma função crescente e o logaritmo é uma operação monotônica, isto é, os parâmetros que maximizam a função original também maximizam a verossimilhança e por consequência o "log-likelihood". O algoritmo de Maximização de Expectativas foi explicado e batizado em um famoso artigo de 1977 por Arthur Dempster, Nan Laird, e Donald Rubin.