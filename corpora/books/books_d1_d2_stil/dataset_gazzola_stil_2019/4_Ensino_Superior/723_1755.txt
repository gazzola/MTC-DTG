 Processamento de Dados Massivos/Introdução
= Introdução =
A evolução dos sistemas computacionais e a difusão do acesso à Internet em diversas formas estão dando origem ao que muitos denominam “a Idade da Informação” . Os volumes massivos de dados produzidos pelas mais diversas fontes, como medições coletadas por sensores dos mais diversos tipos, registros ("logs") dos serviços oferecidos pela Web, diversos tipos de conteúdo produzidos pelos usuários no que se convencionou chamar de Web 2.0, tudo isso apresenta novos desafios e possibilidades. As possibilidades são inúmeras, sempre baseadas na ideia de se derivar novo conhecimento a partir da análise de tais volumes de dados. Os desafios, por sua vez, envolvem questões como definir formas eficientes de coletar e armazenar toda informação, garantir sua preservação e acesso eficientes, bem como extrair informação útil de tais volumes de dados. Dado o interesse geral devido ao seu potencial, essa área vem recebendo grande atenção na imprensa técnica e mesmo na imprensa em geral, sendo denominada “"Big-data"”. Neste trabalho, pretendemos discutir o problema de como processar esses grandes volumes de dados. Exatamente o que se entende por "big-data" depende bastante do contexto . Apesar de normalmente se associar o conceito apenas a volumes extremamente grandes de dados, na verdade a definição abrange três dimensões, que devem ainda ser consideradas em perspectiva para cada usuário: volume, velocidade e variedade . Certamente, volume é uma dimensão claramente associada a dados massivos. Um infográfico produzido por GOOD, Oliver Munday e IBM representa alguns desses volumes. Entre eles, pode-se ver que a cada minuto são carregados no Youtube o equivalente a 20 horas de vídeo, que há em média 50 milhões de tweets por dia e que 2,9 milhões de mensagens de e-mail são enviados por segundo. Entretanto, a definição de volume massivo deve ser também ajustada em função dos recursos disponíveis para seu processamento. Nem todas as organizações possuem os recursos computacionais de uma empresa como Google ou Facebook; em muitos casos, dados na casa de centenas de Gigabytes já apresentam um desafio para serem processados, considerando-se os recursos disponíveis.