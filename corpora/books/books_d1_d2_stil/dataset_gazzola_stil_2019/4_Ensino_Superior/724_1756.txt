 Uma segunda dimensão é a velocidade com que os dados são gerados e com que precisam ser processados em diversos casos. Por exemplo, o Observatório da Web coleta um grande volume de informações em tempo real de diversas fontes, como sítios de notícias, "blogs" e "Twitter", para gerar diversas análises atualizadas a cada minuto durante eventos transmitidos ao vivo. Nesse caso, o desafio é processar o volume de dados gerado ao longo do tempo e em tempo hábil, o que é denominado processamento de "streams". Finalmente, a variedade dos dados e dos resultados esperados também são determinantes para a definição de "big-data". A possibilidade de se coletar informações textuais, fotos, áudio e vídeo tornam muitas vezes inviável o uso de sistemas de gerência de bancos de dados tradicionais. A exploração de informações de redes complexas, representando relacionamentos entre pessoas e/ou eventos dá origem a grafos complexos, que também não são facilmente armazenados em sistemas convencionais. Enfim, o processamento de dados massivos de forma eficiente exige o uso de paralelismo, tanto para o armazenamento dos dados quanto para seu processamento. Dessa forma, o acesso aos dados é acelerado, já que leituras em paralelo se tornam possíveis, e o processamento é dividido entre diversas unidades de processamento, acelerando a geração de respostas. Esse modelo de paralelismo, usualmente conhecido como o padrão "dividir-para-conquistar", ou "divisão-e-conquista". Apesar desse modelo de processamento já ser largamente conhecido da comunidade de processamento paralelo, ele também vem ganhando larga aceitação nas tarefas de "big-data" pelo surgimento de ambientes de processamento desenvolvidos especificamente para esse tipo de atividade. Além disso, esses grandes volumes de dados surgem normalmente no contexto de aplicações em nuvem, que executam em grandes "datacenters", onde recursos para armazenamento e processamento distribuído já existem na forma de um grande número de máquinas convencionais interligadas por redes de alta velocidade. Considerando todos esses fatores, o restante deste livro abordará os elementos principais para viabilizar o processamento de dados massivos. Na seção 2, o ambiente dos "datacenters" atuais é discutido em mais detalhes, para caracterizar melhor as restrições e demandas impostas sobre os ambientes de execução. Um aspecto essencial associado ao ambiente de execução é o sistema de armazenamento dos dados, que também será discutido. Com base nessa análise, a seção 3 discute os desafios enfrentados para se garantir o alto desempenho nesse ambiente. Em seguida, a seção 4 introduz o modelo de programação MapReduce, que se tornou um dos mais populares na área, e a seção 5 dá detalhes do Hadoop, a principal implementação do modelo. Apesar de sua popularidade, entretanto, MapReduce e Hadoop não são a solução para todos os problemas. A seção 6 apresenta ambientes desenvolvidos para facilitar o processamento de certos tipos de dados e certos tipos de algoritmos que não se adaptam bem ao modelo MapReduce. Ainda nesse sentido, a seção 7 descreve uma metodologia para o desenvolvimento de aplicações "big-data" e traz diversos estudos de caso desenvolvidos pelos alunos da disciplina Processamento de Dados Massivos do Departamento de Ciência da Computação da Universidade Federal de Minas Gerais (DCC/UFMG). Finalmente, a seção 8 apresenta algumas considerações finais.