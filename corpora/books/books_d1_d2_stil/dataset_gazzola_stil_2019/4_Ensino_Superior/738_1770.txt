 We cant allow to loose a stream due to a machine crash or a network failure. The main parallelism opportunities in this project is the possibility to include other activities running while the preprocessing run. And this is quite easy to be done using Storm framework. In here, I am showing a simple application that 
The Geeser Project propose several stages. In each stage, a different activity is added to work on the stream parallely. For instance, word count and trending topics on second stage and entity disambiguation on the third stage. For each stage, I propose to write a set of spouts and bolts that are necessary to reach the corresponding objective. That way, developers can mount a topology according their necessity:
The communication will be fully detailed in the project final API. It is based on the non-structured database JSON. This is a standard communication on Storms protocol, which is based only on tuples. This is a simple protocol and ideal to work on databases which tends to have several processing nodes that are independent. Basically, the main topology proposed on the first stage is composed of a single spout which reads the tweet stream from a file to simulate a connection to Twitters API. This was the only option since the virtual machines had only 10 GB of space. The following bolts, projects the full JSON tweet to comunicate only the text of the message to the next bolt. This following bolt will process those texts. The Storm nodes need to implement some functions to work:
Each spout and bolt is a class. Multilang bolts may be implemented using classes that inherits ShelBolt, which runs the bolt in a virtual shell. The comunication is implemented by the mother class. Usually, we use BaseBasicBolt and BaseRichSpout for implementing bolts and spouts. In those cases, we have to implement some functions that concerns the activity done by the bolt or spout.